{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f29b36-aaba-4e5c-a3b6-186a7c021da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample_data.txt'}, page_content='Defining Large Language Models (LLMs)\\nAt their core, Large Language Models are advanced AI systems designed to understand, generate, and work with human language in a way that’s context-aware. Built using massive neural networks and trained on enormous datasets, these models can summarize,  parse and produce text that’s both coherent and contextually relevant, in response to user prompts.\\n\\nLarge Language Models\\n\\nEvolution and Significance of LLMs in AI\\n\\nLLMs are the latest step in an ongoing evolution in machine learning, deep learning and natural language processing, building on Google’s 2017 introduction of transformer models. First described in the paper, “Attention is all you need”, this architecture laid the groundwork for the development of models like OpenAI’s GPT, Google’s BERT, and many of the open-source models on HuggingFace.\\nLLMs are an important tool for modern application development, as they’re the foundation of artificial intelligence (AI) systems that can more naturally interact with humans, giving us new ways to automate tasks, and even entertain ourselves. In the realm of NLP, they have set new benchmarks in translation, summarization, and question-answering, going a long way to provide a new set of natural user interface technologies.\\nThe development of LLMs has continued the renaissance in AI research, with new areas to investigate. Along with other foundational models, they are being used to build new AI-based applications, for research and for industry and consumers. have become a foundation upon which new AI theories and applications are being built, at the same time as forcing us to ask questions about the role of AI in the wider world, and how it might affect society at large.\\nLarge Language Models stand as an important step in the journey to AI, offering a pointer towards more intelligent, intuitive, and interactive systems. As we delve deeper into the mechanics, applications, and implications of LLMs, we stand on the brink of a new era in AI.\\n\\nHow Large Language Models Work\\n\\nThe Fundamentals of Language Models\\nLanguage models, like LLMs, predict the likelihood of a sequence of words based on the patterns they’ve learned from massive text datasets. This approach trains them around syntax, semantics, and context, enabling them to complete sentences, generate coherent paragraphs, or even write entire articles. The training process involves feeding the model examples of text, allowing it to adjust its internal parameters to minimize the difference between its predictions and the actual sequences of words in the training data. This process, known as supervised learning, is iterated millions of times, refining the model’s ability to mimic human language patterns.\\n\\nThe Transformer Architecture: A Breakthrough in LLMs\\nThe transformer model was a major breakthrough in the field of NLP and is the foundation of modern LLMs. Unlike its predecessors, the transformer uses parallel processing, speeding up training times and enabling them to handle longer sequences of text. Transformers are characterized by their use of self-attention mechanisms, which allow the model to weigh the importance of different words in a sentence or document, helping process and manage context and relationships between words.\\n\\nUnderstanding Attention Mechanisms and Neural Networks\\nAt the heart of the transformer architecture lies the attention mechanism, a novel approach that enables the model to focus on different parts of the input text when predicting an output. This system imitates how humans focus on details while ignoring the rest improving the model’s understanding of language structures. Specifically, self-attention enables the model to analyze the input sequence at once assessing the relevance of all words (presented in syllable units) to each other within a sentence or, across multiple sentences. Deep learning networks provide the foundation for Large Language Models (LLMs) consisting of interconnected layers that perform specific data transformations. These networks help the model identify patterns, from syntax at levels to intricate semantics at higher levels. The combination of network depth and breadth with transformer architecture empowers LLMs to process and produce language with sophistication to human linguistic abilities. By integrating language models, transformer design, attention mechanisms and neural networks LLMs realize advancements in machine learning for natural language processing applications. Exploring the potential and limitations of LLMs reveals their contribution, towards emulating intelligence.\\n\\nExploring Major Language Models\\nOverview of the GPT (Generative Pre-trained Transformer) Series\\nThe Generative Pre-trained Transformer series, including GPT 3 and 4 developed by OpenAI is considered at the forefront of Language Models. The series originated with GPT, one of the transformers trained on an amount of text data, from the internet. GPT demonstrated AIs ability to generate contextually relevant text based on given prompts showcasing its proficiency in understanding and producing language.\\nWith each version like GPT 2, GPT 3 and now GPT 4, OpenAI has significantly scaled up both the model size and training data volume. This enhancement has boosted the models’ analytical capabilities. For instance, GPT 3 boasts over 175 billion parameters enabling it to possess a level of comprehension and versatility for tasks ranging from crafting essays and poems to generating code.\\nThe impact of the GPT series goes beyond technological achievements; it has ignited discussions regarding AI’s potential to replicate human creativity with AI-generated content as well as ethical implications shaping the future landscape of human AI interactions.\\n\\nBERT (Bidirectional Encoder Representations from Transformers) by Google introduces an approach, for leveraging context in language models.\\n\\nBERT bidirectional training method enables it to grasp the meaning of a word by considering all the words, around it, unlike models that could only understand context in one direction at a time. This approach has significantly improved performance across natural language processing tasks setting benchmarks for how machines comprehend text.\\n\\nThe impact of BERT is clear from its use in applications ranging from enhancing search engine results to improving language translation services. Its ability to decipher the intent behind search queries has proven invaluable in enhancing user experience online. BERT has now become a model for research leading to the creation of many variations and adaptations tailored to specific languages and tasks.\\n\\nT5 (Text-to-Text Transfer Transformer) and Beyond\\nThe T5 model, also known as Text to Text Transfer Transformer presents a framework developed by Google Research that treats every NLP problem as a text-to-text challenge. This strategy simplifies the utilization of LLMs across tasks by maintaining an input-output format whether it involves translation, summarization, question answering or classification. This streamlined approach benefits both training and deployment processes, for LLMs.\\n\\nT5’s thorough training routine involves a combination of learning methods; unsupervised learning from a text database supervised learning from labeled datasets, for tasks and multitask learning across various fields. This approach contributes to its performance in natural language processing.\\n\\nLooking forward it is evident that the progress of Language Models (LLMs) is ongoing. The field is continuously expanding, with researchers and developers exploring possibilities. The upcoming generation of LLMs is expected to exhibit advanced capabilities in comprehending and generating human language closer integration with other AI sectors like computer vision and robotics and improved mechanisms to address ethical and societal concerns.\\n\\nThe use of Language Models has brought advancements across multiple domains, particularly in Natural Language Processing (NLP). These models have not only elevated the standards in NLP tasks but have also opened doors to novel applications. This section discusses the applications of LLMs emphasizing their influence on natural language comprehension, generation, translation and creative writing.\\n\\nBy leveraging Language Models (LLMs) machines have been empowered with enhanced proficiency, in understanding and processing language.\\n\\nBy harnessing data and employing neural network structures these models are capable of working with the intricacies, context and nuances of language allowing them to carry out intricate natural language processing tasks effectively. Functions, like analyzing sentiments, identifying named entities and categorizing topics have shown enhancements, empowering businesses and researchers to extract insights from textual data more efficiently than ever before.\\n\\nMoving from Text Generation to Language Translation\\nA notable application of Language Models (LLMs) lies in text generation, where models such as GPT 3 have demonstrated the ability to generate contextually relevant text in various styles and formats. This feature holds implications ranging from creating content to generating code and automating programming tasks. Additionally, LLMs can be utilized for language translation by leveraging the structure of languages to provide translations that capture the nuances and cultural context of the text.\\n\\nAIs Role in Creative Writing and Content Generation\\nOne use case for LLMs is, in creative writing and content creation. These models have proven their capability to produce stories, poems and even music. This opens up possibilities for LLMs to serve as tools that inspire ideas help overcome writer’s block or offer alternative perspectives.\\n\\nIn the realm of content creation, Large Language Models (LLMs) are being utilized to generate types of written material on a scale ranging from news articles, to personalized marketing content. This significantly reduces the time and effort needed for content creation.\\n\\nExpanding Boundaries; Moving Beyond Language\\nThe scope of LLMs extends beyond tasks solely related to language. Their capacity to comprehend and produce text that resembles writing is being applied in assistants enhancing customer service through automated personalized interactions. In academia, these models are aiding researchers and academics by summarizing literature and even drafting research papers.\\n\\nAs we delve deeper into exploring the capabilities of Language Models it becomes evident that their influence reaches beyond the realm of Natural Language Processing (NLP). By transforming how we engage with language create content and process information LLMs are paving the way for opportunities for innovation and collaboration across various fields. As these models progress, so will their diverse applications.\\n\\nUtilizing Language Models in Development\\nWith these robust models becoming more accessible grasping how to effectively incorporate them into your code is increasingly crucial. This segment provides guidance on establishing a development environment for LLMs refining techniques, for use cases and integrating these models into applications.\\n\\nCreating an Environment, for Developing Language Models (LLMs)\\nThe initial step in constructing and training your LLMs for project development involves establishing an environment that can support the training and deployment of large-scale models. Cloud computing platforms such as Amazon Web Services (AWS) Google Cloud Platform (GCP) and Microsoft Azure provide the infrastructure offering high-performance computing resources and scalable storage solutions. These platforms often come equipped with configured environments tailored for machine learning and AI development making the setup process more efficient.\\n\\nMoreover utilizing frameworks like TensorFlow or PyTorch can streamline the development process by providing libraries and tools optimized for machine learning including support for LLMs. Tools like Docker and Kubernetes for containerization can also play a role in creating scalable environments suitable for deployment at various stages of development.\\n\\nCustomizing Techniques for Specific Use Cases\\nAlthough pre-trained LLMs such as GPT 3 or BERT come with capabilities out of the box fine tuning these models with domain-specific data can significantly enhance their performance in specific use cases. Tuning entails continuing the training of a trained model on a smaller dataset specific to a particular domain. This approach enables the model to adjust its parameters to comprehend and generate text to the specified domain or task, at hand.\\n\\nFine-tuning effectively requires a curated dataset that accurately mirrors the target domain or task. It’s crucial to maintain a dataset to prevent biases that could impact the model’s performance. Methods, like transfer learning, which involves adapting a model trained for one task to another task can improve the tuning process particularly when dealing with limited data.\\n\\nIncorporating Language Models (LLMs) into Web and Software Applications\\nLLMs offer applications ranging from automated customer service chatbots and personalized content suggestions to data analysis tools. APIs play a role in this integration by connecting applications to LLMs hosted on cloud platforms or local servers.\\n\\nFor web applications, JavaScript libraries like TensorFlow.js enable the deployment of machine learning models in browsers facilitating real-time interactions with LLMs. On the server side frameworks such as Flask, for Python can establish API endpoints that web applications can utilize to access LLM features.\\n\\nWhen integrating LLMs into applications it’s essential to focus on providing users with a coherent experience when interacting with the model within your application. Monitoring the model’s performance and continuously refining its responses based on user input enables you to optimize it for use.\\n\\nBy establishing a workspace for development customizing models, for scenarios and strategically incorporating them into applications developers can utilize Language Models (LLMs) in their programming. Keeping up to date with the advancements. Recommended approaches, in LLM development as the industry progresses can lead to the creation of improved applications and user interactions.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"sample_data.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe921fa-2d22-4b77-9f27-b9cab9d93beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_data.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9046b482-1a97-48c4-8aae-c0a30534f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d378b6-5049-4762-9db7-8c8d54d4fd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'movies.csv', 'row': 0}, page_content='movie_id: 101\\ntitle: K.G.F: Chapter 2\\nindustry: Bollywood\\nrelease_year: 2022\\nimdb_rating: 8.4\\nstudio: Hombale Films\\nlanguage_id: 3\\nbudget: 1\\nrevenue: 12.5\\nunit: Billions\\ncurrency: INR'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 1}, page_content='movie_id: 102\\ntitle: Doctor Strange in the Multiverse of Madness\\nindustry: Hollywood\\nrelease_year: 2022\\nimdb_rating: 7\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 200\\nrevenue: 954.8\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 2}, page_content='movie_id: 103\\ntitle: Thor: The Dark World\\nindustry: Hollywood\\nrelease_year: 2013\\nimdb_rating: 6.8\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 165\\nrevenue: 644.8\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 3}, page_content='movie_id: 104\\ntitle: Thor: Ragnarok\\nindustry: Hollywood\\nrelease_year: 2017\\nimdb_rating: 7.9\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 180\\nrevenue: 854\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 4}, page_content='movie_id: 105\\ntitle: Thor: Love and Thunder\\nindustry: Hollywood\\nrelease_year: 2022\\nimdb_rating: 6.8\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 250\\nrevenue: 670\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 5}, page_content='movie_id: 106\\ntitle: Sholay\\nindustry: Bollywood\\nrelease_year: 1975\\nimdb_rating: 8.1\\nstudio: United Producers\\nlanguage_id: 1\\nbudget: Not Available\\nrevenue: Not Available\\nunit: Not Available\\ncurrency: Not Available'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 6}, page_content='movie_id: 107\\ntitle: Dilwale Dulhania Le Jayenge\\nindustry: Bollywood\\nrelease_year: 1995\\nimdb_rating: 8\\nstudio: Yash Raj Films\\nlanguage_id: 1\\nbudget: 400\\nrevenue: 2000\\nunit: Millions\\ncurrency: INR'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 7}, page_content='movie_id: 108\\ntitle: 3 Idiots\\nindustry: Bollywood\\nrelease_year: 2009\\nimdb_rating: 8.4\\nstudio: Vinod Chopra Films\\nlanguage_id: 1\\nbudget: 550\\nrevenue: 4000\\nunit: Millions\\ncurrency: INR'),\n",
       " Document(metadata={'source': 'movies.csv', 'row': 8}, page_content='movie_id: 109\\ntitle: Kabhi Khushi Kabhie Gham\\nindustry: Bollywood\\nrelease_year: 2001\\nimdb_rating: 7.4\\nstudio: Dharma Productions\\nlanguage_id: 1\\nbudget: 390\\nrevenue: 1360\\nunit: Millions\\ncurrency: INR')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=\"movies.csv\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245b750b-8f58-4959-adb7-9c08fec6104a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'K.G.F: Chapter 2', 'row': 0}, page_content='movie_id: 101\\ntitle: K.G.F: Chapter 2\\nindustry: Bollywood\\nrelease_year: 2022\\nimdb_rating: 8.4\\nstudio: Hombale Films\\nlanguage_id: 3\\nbudget: 1\\nrevenue: 12.5\\nunit: Billions\\ncurrency: INR'),\n",
       " Document(metadata={'source': 'Doctor Strange in the Multiverse of Madness', 'row': 1}, page_content='movie_id: 102\\ntitle: Doctor Strange in the Multiverse of Madness\\nindustry: Hollywood\\nrelease_year: 2022\\nimdb_rating: 7\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 200\\nrevenue: 954.8\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'Thor: The Dark World', 'row': 2}, page_content='movie_id: 103\\ntitle: Thor: The Dark World\\nindustry: Hollywood\\nrelease_year: 2013\\nimdb_rating: 6.8\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 165\\nrevenue: 644.8\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'Thor: Ragnarok', 'row': 3}, page_content='movie_id: 104\\ntitle: Thor: Ragnarok\\nindustry: Hollywood\\nrelease_year: 2017\\nimdb_rating: 7.9\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 180\\nrevenue: 854\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'Thor: Love and Thunder', 'row': 4}, page_content='movie_id: 105\\ntitle: Thor: Love and Thunder\\nindustry: Hollywood\\nrelease_year: 2022\\nimdb_rating: 6.8\\nstudio: Marvel Studios\\nlanguage_id: 5\\nbudget: 250\\nrevenue: 670\\nunit: Millions\\ncurrency: USD'),\n",
       " Document(metadata={'source': 'Sholay', 'row': 5}, page_content='movie_id: 106\\ntitle: Sholay\\nindustry: Bollywood\\nrelease_year: 1975\\nimdb_rating: 8.1\\nstudio: United Producers\\nlanguage_id: 1\\nbudget: Not Available\\nrevenue: Not Available\\nunit: Not Available\\ncurrency: Not Available'),\n",
       " Document(metadata={'source': 'Dilwale Dulhania Le Jayenge', 'row': 6}, page_content='movie_id: 107\\ntitle: Dilwale Dulhania Le Jayenge\\nindustry: Bollywood\\nrelease_year: 1995\\nimdb_rating: 8\\nstudio: Yash Raj Films\\nlanguage_id: 1\\nbudget: 400\\nrevenue: 2000\\nunit: Millions\\ncurrency: INR'),\n",
       " Document(metadata={'source': '3 Idiots', 'row': 7}, page_content='movie_id: 108\\ntitle: 3 Idiots\\nindustry: Bollywood\\nrelease_year: 2009\\nimdb_rating: 8.4\\nstudio: Vinod Chopra Films\\nlanguage_id: 1\\nbudget: 550\\nrevenue: 4000\\nunit: Millions\\ncurrency: INR'),\n",
       " Document(metadata={'source': 'Kabhi Khushi Kabhie Gham', 'row': 8}, page_content='movie_id: 109\\ntitle: Kabhi Khushi Kabhie Gham\\nindustry: Bollywood\\nrelease_year: 2001\\nimdb_rating: 7.4\\nstudio: Dharma Productions\\nlanguage_id: 1\\nbudget: 390\\nrevenue: 1360\\nunit: Millions\\ncurrency: INR')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=\"movies.csv\", source_column=\"title\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64cbc8a-6b74-4832-a4a9-48d7da4d559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in /opt/anaconda3/lib/python3.12/site-packages (0.15.1)\n",
      "Requirement already satisfied: libmagic in /opt/anaconda3/lib/python3.12/site-packages (1.0)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.0.0)\n",
      "Requirement already satisfied: filetype in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (5.2.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (2.12.1)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (2024.4.27)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (3.9.6)\n",
      "Requirement already satisfied: backoff in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.11.0)\n",
      "Requirement already satisfied: unstructured-client in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (0.25.4)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (1.14.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (5.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->unstructured) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk->unstructured) (2023.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->unstructured) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->unstructured) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->unstructured) (2024.7.4)\n",
      "Requirement already satisfied: deepdiff>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (7.0.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (23.2)\n",
      "Requirement already satisfied: pypdf>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (4.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "#installing necessary libraries, libmagic is used for file type detection\n",
    "!pip3 install unstructured libmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36dc9e24-0b44-4a8f-b811-bc8989e56d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "loader = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html\",\n",
    "        \"https://www.moneycontrol.com/news/business/markets/market-corrects-post-rbi-ups-inflation-forecast-icrr-bet-on-these-top-10-rate-sensitive-stocks-ideas-11142611.html\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e2720a-a5cb-496a-be01-f068e75f5712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b0fc28-5c03-47e0-92d2-7973bf015f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English\\n\\nHindi\\n\\nGujarati\\n\\nSpecials\\n\\nHello, Login\\n\\nHello, Login\\n\\nLog-inor Sign-Up\\n\\nMy Account\\n\\nMy Pro'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3bd1a43-7ee0-47b0-b6d8-7412b6d0ccdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3eb282-f484-47b2-ac97-23d7a29aa5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \n",
    "Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007 and was originally set to be directed by Steven Spielberg. \n",
    "Kip Thorne, a Caltech theoretical physicist and 2017 Nobel laureate in Physics,[4] was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar. \n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm. Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles. \n",
    "Interstellar uses extensive practical and miniature effects, and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered in Los Angeles on October 26, 2014. In the United States, it was first released on film stock, expanding to venues using digital projectors. The film received generally positive reviews from critics and grossed over $677 million worldwide ($715 million after subsequent re-releases), making it the tenth-highest-grossing film of 2014. \n",
    "It has been praised by astronomers for its scientific accuracy and portrayal of theoretical astrophysics.[5][6][7] Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e82bc0af-e56e-4268-8bad-031ff54a7fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher N'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "318d7276-4979-4505-9b74-abd4e00ce21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(\" \")\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d081923-84f2-4a76-94e6-d6d3d87f9661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \\nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt ',\n",
       " 'Damon, and Michael Caine. \\nSet in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "\n",
    "s = \"\"\n",
    "for word in words:\n",
    "    s += word + \" \"\n",
    "    if len(s)>200:\n",
    "        chunks.append(s)\n",
    "        s = \"\"\n",
    "        \n",
    "chunks.append(s)\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9355d06a-7f0f-4d5a-80b6-06ec0e63b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 358, which is longer than the specified 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0097202-8532-461a-a1fc-a92d7a33f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "120\n",
      "210\n",
      "181\n",
      "197\n",
      "207\n",
      "128\n",
      "357\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1714bc0f-0cb7-41d2-8998-03ebccf8efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \"],  # List of separators based on requirement (defaults to [\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunk_size = 200,  # size of each chunk created\n",
    "    chunk_overlap  = 0,  # size of  overlap between chunks in order to maintain the context\n",
    "    length_function = len  # Function to calculate size, currently we are using \"len\" which denotes length of string however you can pass any token counter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0fe3be6-711b-4f0c-ba2a-d54a51302172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. ',\n",
       " 'It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. ',\n",
       " 'Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_split = text.split(\"\\n\\n\")[0]\n",
    "first_split\n",
    "len(first_split)\n",
    "\n",
    "second_split = first_split.split(\"\\n\")\n",
    "second_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0456ff1-a03e-4d2b-af0e-e7b84f81e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "121\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "for split in second_split:\n",
    "    print(len(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20c18395-a720-47c4-9de8-079711c4353b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_split[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
